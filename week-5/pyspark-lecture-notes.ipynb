{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b48abc",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714a65cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f9bc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 08:14:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create spark session:\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5745ded1",
   "metadata": {},
   "source": [
    "## Check Spark is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d66210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to file\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"./data/taxi+_zone_lookup.csv\")\n",
    "\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8ca8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1e5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "df.write.parquet(\"./data/taxi_zones\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a4b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "-rw-r--r--  1 vgarist  staff     0 Mar  3 08:15 _SUCCESS\r\n",
      "-rw-r--r--  1 vgarist  staff  5916 Mar  3 08:15 part-00000-8385dc98-c529-47ec-a1c4-a227a1b2520a-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "ls -l ./data/taxi_zones/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88888eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+------------+\n",
      "|LocationID|Borough|          Zone|service_zone|\n",
      "+----------+-------+--------------+------------+\n",
      "|         1|    EWR|Newark Airport|         EWR|\n",
      "|         2| Queens|   Jamaica Bay|   Boro Zone|\n",
      "+----------+-------+--------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# could be easily read from parquet:\n",
    "spark.read.parquet(\"./data/taxi_zones\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e568c6c",
   "metadata": {},
   "source": [
    "## First look at Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfdae3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b0184",
   "metadata": {},
   "source": [
    "- read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f12eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHV trips:\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"./data/fhvhv_trips/fhvhv_tripdata_2021-06.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6db5434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14961892"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d86a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37fafc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from spark to pandas:\n",
    "df_pandas = df.limit(1000).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c09a48",
   "metadata": {},
   "source": [
    "- update schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44aee3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# could move from pandas to Spark\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a31536f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update FHV trips:\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"./data/fhvhv_trips/fhvhv_tripdata_2021-06.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db4d78b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 2, 41), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 7, 46), PULocationID=174, DOLocationID=18, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 16, 16), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 21, 14), PULocationID=32, DOLocationID=254, SR_Flag='N', Affiliated_base_number='B02764')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all columns were parsed successfully:\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb6a021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition for further saving\n",
    "df = df.repartition(numPartitions=12)  # nothing has changed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d15059e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 08:16:06 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save partitioned DataFrame:\n",
    "df.write.parquet(\"./data/fhvhv_trips/2021/06/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97a852c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 591872\r\n",
      "-rw-r--r--  1 vgarist  staff     0B Mar  3 08:16 _SUCCESS\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00000-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00001-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00002-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00003-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00004-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00005-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00006-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00007-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00008-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00009-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00010-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 08:16 part-00011-c18a05d6-ebd2-4e74-ad7a-2613b288c1d7-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh {\"./data/fhvhv_trips/2021/06/\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba72159",
   "metadata": {},
   "source": [
    "## Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8954d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2be7f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02875|2021-06-16 22:08:45|2021-06-16 22:38:10|          48|         181|      N|                B02875|\n",
      "|              B02875|2021-06-27 08:13:22|2021-06-27 08:16:18|          10|          10|      N|                B02875|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data:\n",
    "df = spark.read.parquet(\"./data/fhvhv_trips/2021/06/\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9aa99b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-06-16 22:08:45|2021-06-16 22:38:10|          48|         181|\n",
      "|2021-06-27 08:13:22|2021-06-27 08:16:18|          10|          10|\n",
      "|2021-06-14 07:12:35|2021-06-14 07:46:52|         112|         166|\n",
      "|2021-06-06 04:05:40|2021-06-06 04:18:32|          79|         261|\n",
      "|2021-06-17 09:17:20|2021-06-17 09:48:09|          68|         138|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter data (first TRANSFORMATION, then ACTION)\n",
    "\n",
    "# Transformations: select, filter, join, groupby\n",
    "# Actions: show, take, head, write, ...\n",
    "df \\\n",
    "    .select(\"pickup_datetime\", \"dropoff_datetime\", \"PULocationID\", \"DOLocationID\") \\\n",
    "    .filter(df[\"Affiliated_base_number\"] == \"B02875\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48f9b538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-06-16|  2021-06-16|          48|         181|\n",
      "| 2021-06-27|  2021-06-27|          10|          10|\n",
      "| 2021-06-13|  2021-06-13|          89|         189|\n",
      "| 2021-06-15|  2021-06-15|          36|          82|\n",
      "| 2021-06-09|  2021-06-09|         254|         254|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# option #1: use predefined functions (F):\n",
    "df \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(df[\"pickup_datetime\"])) \\\n",
    "    .withColumn(\"dropoff_date\", F.to_date(df[\"dropoff_datetime\"])) \\\n",
    "    .select(\"pickup_date\", \"dropoff_date\", \"PULocationID\", \"DOLocationID\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "065eb58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option #2: use UDFs\n",
    "def process_location(location_id: int) -> str:\n",
    "    \"\"\"Assign City according to location\"\"\"\n",
    "    if 0 < location_id <= 100:\n",
    "        return \"Moscow\"\n",
    "    elif 100 < location_id <= 200:\n",
    "        return \"London\"\n",
    "    else:\n",
    "        return \"Tokyo\"\n",
    "\n",
    "\n",
    "# check UDF works\n",
    "process_location_udf = F.udf(process_location, returnType=types.StringType())\n",
    "df \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(df[\"pickup_datetime\"])) \\\n",
    "    .withColumn(\"dropoff_date\", F.to_date(df[\"dropoff_datetime\"])) \\\n",
    "    .withColumn(\"pickup_location\", process_location_udf(df[\"PULocationID\"])) \\\n",
    "    .withColumn(\"dropoff_location\", process_location_udf(df[\"DOLocationID\"])) \\\n",
    "    .select(\"pickup_date\", \"dropoff_date\", \"pickup_location\", \"dropoff_location\") \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34bd2930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------------+-------------------+--------+\n",
      "|pickup_date|dropoff_date|pickup_location_id|dropoff_location_id|base_num|\n",
      "+-----------+------------+------------------+-------------------+--------+\n",
      "| 2021-06-16|  2021-06-16|                48|                181|  b02875|\n",
      "| 2021-06-27|  2021-06-27|                10|                 10|  b02875|\n",
      "| 2021-06-13|  2021-06-13|                89|                189|    null|\n",
      "| 2021-06-15|  2021-06-15|                36|                 82|  b02764|\n",
      "| 2021-06-09|  2021-06-09|               254|                254|    null|\n",
      "+-----------+------------+------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# option #3: SQL queries\n",
    "df.createOrReplaceTempView(name=\"fhv_trips\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    date(pickup_datetime) as pickup_date,\n",
    "    date(dropoff_datetime) as dropoff_date,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    lower(Affiliated_base_number) as base_num\n",
    "FROM fhv_trips\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40b9b9",
   "metadata": {},
   "source": [
    "## SQL with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d92fa498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stats = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    date(date_trunc('WEEK', pickup_datetime)) as d_month,\n",
    "    count(*) as trip_cnt\n",
    "FROM fhv_trips\n",
    "WHERE SR_Flag = 'N'\n",
    "GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56bd50e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save query result to file:\n",
    "df_stats.write.parquet(\"./data/fhvhv_trips/weekly_dynamics\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d7453d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|   d_month|trip_cnt|\n",
      "+----------+--------+\n",
      "|2021-06-28| 1377049|\n",
      "|2021-05-31| 3061878|\n",
      "|2021-06-14| 3488097|\n",
      "|2021-06-07| 3524541|\n",
      "|2021-06-21| 3506584|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check everything was saved correctly and could be read:\n",
    "spark.read.parquet(\"./data/fhvhv_trips/weekly_dynamics/\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
