{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6fc4e18",
   "metadata": {},
   "source": [
    "# Week #5. Batch Processing with Pyspark\n",
    "\n",
    "## Home Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98ad5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b5272",
   "metadata": {},
   "source": [
    "### Load data from Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c2f1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_data_url = \\\n",
    "    \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c2ca1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_data_dir = \"data/fhvhv_trips\"\n",
    "fhvhv_data_filepath = Path(fhvhv_data_dir, Path(fhvhv_data_url).name)  # data is saved here!\n",
    "\n",
    "if not fhvhv_data_filepath.exists():\n",
    "    os.system(f\"wget -P {fhvhv_data_dir} {fhvhv_data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452f51c",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "`3.3.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df12f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init session:\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5869fe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61cdeb",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "`24M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ef75501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17025fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FHV trips\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(str(fhvhv_data_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcbfe0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 18:02:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# repartition & save to .parquet\n",
    "df = df.repartition(12)\n",
    "df.write.parquet(\n",
    "    f\"{str(fhvhv_data_dir)}/parquet/2021/06\",\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a61ba229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00000-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00001-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00002-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00003-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00004-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00005-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00006-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00007-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00008-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00009-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00010-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 vgarist  staff    24M Mar  3 18:02 part-00011-47a0b4b3-047c-4550-8ef3-600d22729ca9-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh data/fhvhv_trips/parquet/2021/06/ | grep .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c05e5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02875|2021-06-16 22:08:45|2021-06-16 22:38:10|          48|         181|      N|                B02875|\n",
      "|              B02875|2021-06-27 08:13:22|2021-06-27 08:16:18|          10|          10|      N|                B02875|\n",
      "|              B02510|2021-06-13 22:34:33|2021-06-13 22:53:03|          89|         189|      N|                  null|\n",
      "|              B02764|2021-06-15 11:15:06|2021-06-15 11:35:40|          36|          82|      N|                B02764|\n",
      "|              B02510|2021-06-09 11:41:36|2021-06-09 11:45:32|         254|         254|      N|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check could be read:\n",
    "df = spark.read.parquet(\"./data/fhvhv_trips/parquet/2021/06/\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcaf39e",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "`452,470`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a403df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter & count:\n",
    "df \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(\"pickup_datetime\")) \\\n",
    "    .filter(\"pickup_date == '2021-06-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b229a1",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "`66.87`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46aac394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|max(diff_in_hours)|\n",
      "+------------------+\n",
      "|  66.8788888888889|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn(\"pickup_sec\", F.unix_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_sec\", F.unix_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"diff_in_hours\", (F.col(\"dropoff_sec\") - F.col(\"pickup_sec\")) / 3600) \\\n",
    "    .groupBy() \\\n",
    "    .max(\"diff_in_hours\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee13d7",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Port: `4040`\n",
    "\n",
    "Spark Jobs could be monitored here:<br>\n",
    "`http://localhost:4040/jobs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c89a40",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "`Crown Heights North`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f9073b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from web\n",
    "taxi_zones_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\"\n",
    "taxi_zones_filepath = Path(\"data/taxi_zones\", Path(taxi_zones_url).name)\n",
    "\n",
    "if not taxi_zones_filepath.exists():\n",
    "    os.system(f\"wget -P {str(taxi_zones_filepath.parent)} {fhvhv_data_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae7a502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define zones schema:\n",
    "zones_schema = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), nullable=False),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b43cdb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read zones:\n",
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(zones_schema) \\\n",
    "    .csv(str(taxi_zones_filepath))\n",
    "\n",
    "df_zones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc44d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Views:\n",
    "df.createOrReplaceTempView(\"taxi_trips\")\n",
    "df_zones.createOrReplaceTempView(\"taxi_zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef819301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|           zone_name|tripx_cnt|\n",
      "+--------------------+---------+\n",
      "| Crown Heights North|   231279|\n",
      "|        East Village|   221244|\n",
      "|         JFK Airport|   188867|\n",
      "|      Bushwick South|   187929|\n",
      "|       East New York|   186780|\n",
      "|TriBeCa/Civic Center|   164344|\n",
      "|   LaGuardia Airport|   161596|\n",
      "|            Union Sq|   158937|\n",
      "|        West Village|   154698|\n",
      "|             Astoria|   152493|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:======================================>                  (8 + 4) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get most grequest pickup location zone:\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    taxi_zones.Zone as zone_name,\n",
    "    count(*) as trips_cnt\n",
    "FROM taxi_trips\n",
    "JOIN taxi_zones\n",
    "    ON taxi_trips.PULocationID = taxi_zones.LocationID\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a477e72",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
